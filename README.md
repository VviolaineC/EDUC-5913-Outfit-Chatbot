# EDUC-5913-Outfit-Chatbot
:)

## Introduction

ğŸ’–Hello everyone. The project of Haipei and I wants to establish a vertical chatbot system integrating multiple models for daily clothing matching. We combine text language models and image recognition capabilities to provide real-time feedback based on user-input images, temperatures within a limited time range, and dressing scenarios.

ğŸ’–å¤§å®¶å¥½ï¼Œæˆ‘å’Œæµ·åŸ¹çš„é¡¹ç›®æƒ³è¦å»ºç«‹ä¸€ä¸ªé›†æˆå¤šç§æ¨¡å‹çš„å‚ç›´é¢†åŸŸèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºæ—¥å¸¸æœè£…æ­é…ã€‚æˆ‘ä»¬ç»“åˆäº†æ–‡æœ¬è¯­è¨€æ¨¡å‹ä»¥åŠå›¾ç‰‡è¯†åˆ«èƒ½åŠ›ï¼Œä¾æ®ç”¨æˆ·è¾“å…¥çš„å›¾ç‰‡ã€é™å®šæ—¶é—´èŒƒå›´å†…çš„æ°”æ¸©ã€ç©¿æ­æ‰€éœ€åœºæ™¯æ¥è¿›è¡Œå®æ—¶åé¦ˆã€‚ 

ğŸ˜‡Since there are two of us writing code simultaneously, maintenance and collaborative construction are of great significance in code editing. For example, every time we add a new model type, we need to make manual modifications in multiple parts of the code, which makes it easy to miss something or introduce errors. 

ğŸ˜‡å› ä¸ºæˆ‘ä»¬æ˜¯ä¸¤ä¸ªäººåŒæ—¶å†™ä½œï¼Œæ‰€ä»¥ç»´æŠ¤å’Œå…±å»ºåœ¨ä»£ç ç¼–è¾‘ä¸­æ˜¾å¾—ååˆ†é‡è¦ã€‚è­¬å¦‚è¯´ï¼Œ å½“æˆ‘ä»¬æ¯æ¬¡æ–°å¢ä¸€ä¸ªæ¨¡å‹ç±»å‹ï¼Œéƒ½éœ€è¦åœ¨ä»£ç çš„å¤šå¤„æ‰‹åŠ¨ä¿®æ”¹ï¼Œå®¹æ˜“é—æ¼æˆ–å¼•å…¥é”™è¯¯ã€‚

## Potential issues
### ğŸ¤¯Potential Scenario 1 æ½œåœ¨åœºæ™¯1 
```python
# new additional model "gpt5"
if model_type == "gpt5":
print("Running GPT-5")
```
Issues é—®é¢˜æ‰€åœ¨:

â€¢ If these strings are used in dozens of places, we need to check one by one to see if they have all been updated.

â€¢ Without centralized management, it's very easy to miss something or cause logical conflicts. 

â€¢	å¦‚æœæœ‰å‡ åå¤„åœ°æ–¹ä½¿ç”¨äº†è¿™äº›å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬éœ€è¦é€ä¸€æ£€æŸ¥æ˜¯å¦éƒ½æ›´æ–°äº†ã€‚

â€¢	æ²¡æœ‰é›†ä¸­ç®¡ç†æ—¶ï¼Œå¾ˆå®¹æ˜“é—æ¼æˆ–å¯¼è‡´é€»è¾‘å†²çªã€‚

### ğŸ¤¯Potential Scenario 2 æ½œåœ¨åœºæ™¯2

When I want to deploy a local open-source model from Hugging Face by myself, my team doesn't recognize this small self-owned model. Using strings directly can't provide any context information, and at this time, the meaning of the code isn't clear enough. 

å½“æˆ‘æƒ³è¦è‡ªå·±éƒ¨ç½²ä¸€ä¸ªhugging face ä¸Šçš„æœ¬åœ°å¼€æºæ¨¡å‹ï¼Œ æˆ‘çš„team ä¸è®¤è¯†è¿™ä¸ªè‡ªæœ‰å°æ¨¡å‹ï¼Œ ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²æ— æ³•æä¾›ä»»ä½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ­¤æ—¶ä»£ç çš„å«ä¹‰ä¸å¤Ÿæ¸…æ™°ã€‚

```python
if model_type == "llama3":
print("Model is LLaMA-3")
```

Issues é—®é¢˜æ‰€åœ¨:

â€¢	What is "llama3"? Is it a variable? Or a specific value?

â€¢	"llama3" æ˜¯ä»€ä¹ˆï¼Ÿä¸€ä¸ªå˜é‡ï¼Ÿè¿˜æ˜¯ä¸€ä¸ªå…·ä½“çš„å€¼ï¼Ÿ

**âš ï¸Therefore, during the process of writing code, what we consider is not just the construction of the system. How to make our existing system more semantic (clear), safer and easier to maintain is of crucial importance. Hence, in the early stage of building the system, we introduced "Enum". Although the following piece of code is simple, its advantages will become more and more obvious as the program becomes more complex.**

**âš ï¸å› æ­¤ï¼Œåœ¨å†™ä»£ç çš„è¿‡ç¨‹ä¸­ï¼Œ æˆ‘ä»¬è€ƒè™‘çš„ä¸ä»…ä»…æ˜¯ç³»ç»Ÿçš„æ„å»ºï¼Œ å¦‚ä½•å°†æˆ‘ä»¬ç°æœ‰ç³»ç»Ÿæ„å»ºå¾—æ›´è¯­ä¹‰åŒ–ï¼ˆæ¸…æ™°ï¼‰ã€æ›´å®‰å…¨ã€æ›´æ˜“ç»´æŠ¤ï¼Œè‡³å…³é‡è¦ã€‚ æ‰€ä»¥ï¼Œåœ¨æ„å»ºç³»ç»Ÿçš„å‰æœŸï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œEnumâ€ã€‚ 
ä»¥ä¸‹æ®µä»£ç è™½ç„¶ç®€å•ï¼Œä½†å®ƒçš„å¥½å¤„ä¼šåœ¨ç¨‹åºå˜å¤æ‚æ—¶è¶Šæ¥è¶Šæ˜æ˜¾ã€‚**


```python
class ModelType(Enum):
    """Supported model types"""
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
```
## So, What is the Benefit of Using Enum?

ğŸ§ Define a set of fixed model types, which represent a set of options that won't change randomly.

ğŸ§ å®šä¹‰ä¸€ç»„å›ºå®šçš„æ¨¡å‹ç±»å‹ï¼Œè¿™äº›æ¨¡å‹ç±»å‹ä»£è¡¨äº†ä¸€ç»„ä¸ä¼šéšæ„å˜åŒ–çš„é€‰é¡¹ã€‚

ğŸ¤™ğŸ»Improve the readability of code

ğŸ¤™ğŸ»æé«˜ä»£ç çš„å¯è¯»æ€§

ğŸ‘ŒğŸ»Improve code security

ğŸ‘ŒğŸ»æé«˜ä»£ç å®‰å…¨æ€§

Besides the issue of string spelling, when using enums, IDEs or code checking tools will automatically prompt errors.

é™¤äº†å­—ç¬¦ä¸²æ‹¼å†™çš„é—®é¢˜ï¼Œ ç”¨æšä¸¾æ—¶ï¼ŒIDE æˆ–ä»£ç æ£€æŸ¥å·¥å…·ä¼šè‡ªåŠ¨æç¤ºé”™è¯¯
```python
if model_type == ModelType.GPT5:  # If GPT5 is not defined, an error will be reported here.
print("This is GPT-4")
```

ğŸ˜More convenient for expansion and maintenance.

ğŸ˜æ›´æ–¹ä¾¿çš„æ‰©å±•å’Œç»´æŠ¤

Suppose a new model needs to be added in the future, such as "gpt5". You can directly add a new member to the enumeration class: 

å‡è®¾ä»¥åéœ€è¦æ·»åŠ ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œæ¯”å¦‚ "gpt5"ï¼Œå¯ä»¥ç›´æ¥åœ¨æšä¸¾ç±»ä¸­æ–°å¢æˆå‘˜ï¼š

```python
class ModelType(Enum):
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
GPT5 = "gpt5"  # newly added
```

## Scaling 

ğŸ˜†Similarly, the @dataclass we use later is also considered for the same reason: We hope it can simplify class definitions in the code and enhance its maintainability. 

ğŸ˜†åŒç†ï¼Œæˆ‘ä»¬åœ¨ä¹‹åä½¿ç”¨çš„@dataclassä¹Ÿæ˜¯æ‹¥æœ‰åŒæ ·çš„è€ƒè™‘ï¼š å¸Œæœ›ä»£ç ç®€åŒ–ç±»å®šä¹‰ã€å¢å¼ºå…¶å¯ç»´æŠ¤æ€§ã€‚

For example, without @dataclass, we would need to write these methods manually:

ä¾‹å¦‚ï¼Œå¦‚æœæ²¡æœ‰ @dataclassï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ç¼–å†™è¿™äº›æ–¹æ³•ï¼š

```python
class ModelConfig:
    def __init__(self, model_type, max_length=4096, temperature=0.7):
        self.model_type = model_type
        self.max_length = max_length
        self.temperature = temperature

    def __repr__(self):
        return f"ModelConfig(model_type={self.model_type}, max_length={self.max_length}, temperature={self.temperature})"

    def __eq__(self, other):
        if not isinstance(other, ModelConfig):
            return False
        return (self.model_type == other.model_type and
                self.max_length == other.max_length and
                self.temperature == other.temperature)
```

**However, after using @dataclass, the effect is exactly the same and the code is quite concise.**

**ä½†æœ‰äº†@dataclass ä¹‹åï¼Œ æ•ˆæœå®Œå…¨ç›¸åŒï¼Œä»£ç ç›¸å½“ç®€æ´ã€‚**

```python
from dataclasses import dataclass

@dataclass
class ModelConfig:
    model_type: str
    max_length: int = 4096
temperature: float = 0.7
```

## ModelConfig & LLMClient

Since we expect this system to integrate multiple models, we use a unified interface to initialize, manage and utilize various large language models (LLM). Here, we use ModelConfig to store the parameters for model initialization, and the LLMClient is employed to read the content of ModelConfig to load and use the models. 

ç”±äºæˆ‘ä»¬å¸Œæœ›æœ¬ç³»ç»Ÿé›†æˆå¤šç§æ¨¡å‹ï¼Œ å› æ­¤æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£æ¥åˆå§‹åŒ–ã€ç®¡ç†å’Œä½¿ç”¨å„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä½¿ç”¨ModelConfigè¿›è¡Œæ¨¡å‹åˆå§‹åŒ–çš„å‚æ•°å­˜å‚¨ã€ ç”¨LLMClientè¯»å– ModelConfig çš„å†…å®¹æ¥åŠ è½½å’Œä½¿ç”¨æ¨¡å‹ã€‚

```python
class LLMClient:
    """Generic LLM client supporting multiple model types"""
    
    def __init__(self, config: ModelConfig):
        """
        Initialize LLM client with specified configuration
        
        Args:
            config: ModelConfig instance with model settings
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.tokenizer = None
        
        # Initialize the specified model type
        self._initialize_model()
```



