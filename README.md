# EDUC-5913-Outfit-Chatbot
:)

## Introduction

ðŸ’–Hello everyone. The project of Haipei and I wants to establish a vertical chatbot system integrating multiple models for daily clothing matching. We combine text language models and image recognition capabilities to provide real-time feedback based on user-input images, temperatures within a limited time range, and dressing scenarios.

ðŸ’–å¤§å®¶å¥½ï¼Œæˆ‘å’Œæµ·åŸ¹çš„é¡¹ç›®æƒ³è¦å»ºç«‹ä¸€ä¸ªé›†æˆå¤šç§æ¨¡åž‹çš„åž‚ç›´é¢†åŸŸèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºŽæ—¥å¸¸æœè£…æ­é…ã€‚æˆ‘ä»¬ç»“åˆäº†æ–‡æœ¬è¯­è¨€æ¨¡åž‹ä»¥åŠå›¾ç‰‡è¯†åˆ«èƒ½åŠ›ï¼Œä¾æ®ç”¨æˆ·è¾“å…¥çš„å›¾ç‰‡ã€é™å®šæ—¶é—´èŒƒå›´å†…çš„æ°”æ¸©ã€ç©¿æ­æ‰€éœ€åœºæ™¯æ¥è¿›è¡Œå®žæ—¶åé¦ˆã€‚ 

ðŸ˜‡Since there are two of us writing code simultaneously, maintenance and collaborative construction are of great significance in code editing. For example, every time we add a new model type, we need to make manual modifications in multiple parts of the code, which makes it easy to miss something or introduce errors. 

ðŸ˜‡å› ä¸ºæˆ‘ä»¬æ˜¯ä¸¤ä¸ªäººåŒæ—¶å†™ä½œï¼Œæ‰€ä»¥ç»´æŠ¤å’Œå…±å»ºåœ¨ä»£ç ç¼–è¾‘ä¸­æ˜¾å¾—ååˆ†é‡è¦ã€‚è­¬å¦‚è¯´ï¼Œ å½“æˆ‘ä»¬æ¯æ¬¡æ–°å¢žä¸€ä¸ªæ¨¡åž‹ç±»åž‹ï¼Œéƒ½éœ€è¦åœ¨ä»£ç çš„å¤šå¤„æ‰‹åŠ¨ä¿®æ”¹ï¼Œå®¹æ˜“é—æ¼æˆ–å¼•å…¥é”™è¯¯ã€‚

## Potential issues
### ðŸ¤¯Potential Scenario 1 æ½œåœ¨åœºæ™¯1 
```python
# new additional model "gpt5"
if model_type == "gpt5":
print("Running GPT-5")
```
Issues é—®é¢˜æ‰€åœ¨:

â€¢ If these strings are used in dozens of places, we need to check one by one to see if they have all been updated.

â€¢ Without centralized management, it's very easy to miss something or cause logical conflicts. 

â€¢	å¦‚æžœæœ‰å‡ åå¤„åœ°æ–¹ä½¿ç”¨äº†è¿™äº›å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬éœ€è¦é€ä¸€æ£€æŸ¥æ˜¯å¦éƒ½æ›´æ–°äº†ã€‚

â€¢	æ²¡æœ‰é›†ä¸­ç®¡ç†æ—¶ï¼Œå¾ˆå®¹æ˜“é—æ¼æˆ–å¯¼è‡´é€»è¾‘å†²çªã€‚

### ðŸ¤¯Potential Scenario 2 æ½œåœ¨åœºæ™¯2

When I want to deploy a local open-source model from Hugging Face by myself, my team doesn't recognize this small self-owned model. Using strings directly can't provide any context information, and at this time, the meaning of the code isn't clear enough. 

å½“æˆ‘æƒ³è¦è‡ªå·±éƒ¨ç½²ä¸€ä¸ªhugging face ä¸Šçš„æœ¬åœ°å¼€æºæ¨¡åž‹ï¼Œ æˆ‘çš„team ä¸è®¤è¯†è¿™ä¸ªè‡ªæœ‰å°æ¨¡åž‹ï¼Œ ç›´æŽ¥ä½¿ç”¨å­—ç¬¦ä¸²æ— æ³•æä¾›ä»»ä½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ­¤æ—¶ä»£ç çš„å«ä¹‰ä¸å¤Ÿæ¸…æ™°ã€‚

```python
if model_type == "llama3":
print("Model is LLaMA-3")
```

Issues é—®é¢˜æ‰€åœ¨:

â€¢	What is "llama3"? Is it a variable? Or a specific value?

â€¢	"llama3" æ˜¯ä»€ä¹ˆï¼Ÿä¸€ä¸ªå˜é‡ï¼Ÿè¿˜æ˜¯ä¸€ä¸ªå…·ä½“çš„å€¼ï¼Ÿ

**å› æ­¤ï¼Œåœ¨å†™ä»£ç çš„è¿‡ç¨‹ä¸­ï¼Œ æˆ‘ä»¬è€ƒè™‘çš„ä¸ä»…ä»…æ˜¯ç³»ç»Ÿçš„æž„å»ºï¼Œ å¦‚ä½•å°†æˆ‘ä»¬çŽ°æœ‰ç³»ç»Ÿæž„å»ºå¾—æ›´è¯­ä¹‰åŒ–ï¼ˆæ¸…æ™°ï¼‰ã€æ›´å®‰å…¨ã€æ›´æ˜“ç»´æŠ¤ï¼Œè‡³å…³é‡è¦ã€‚ æ‰€ä»¥ï¼Œåœ¨æž„å»ºç³»ç»Ÿçš„å‰æœŸï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œEnumâ€ã€‚ 
ä»¥ä¸‹æ®µä»£ç è™½ç„¶ç®€å•ï¼Œä½†å®ƒçš„å¥½å¤„ä¼šåœ¨ç¨‹åºå˜å¤æ‚æ—¶è¶Šæ¥è¶Šæ˜Žæ˜¾ã€‚**


```python
from typing import Optional
from enum import Enum
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import openai
from dataclasses import dataclass
```
Define the name in case of miswritting
```python
class ModelType(Enum):
    """Supported model types"""
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
```
Class definition using the Python @dataclass decorator for configuring the initialization parameters of an AI model.
```python
class ModelConfig:
    """Configuration for model initialization"""
    model_type: ModelType
    model_path: Optional[str] = None  # Local path or HuggingFace model ID
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    max_length: int = 4096
    temperature: float = 0.7
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
```
Initialize the appropriate model based on configuration
```python
    def _initialize_model(self):
        """Initialize the appropriate model based on configuration"""
        try:
            if self.config.model_type in [ModelType.GPT4, ModelType.GPT35]:
                self._initialize_openai()
            elif self.config.model_type == ModelType.LLAMA3:
                self._initialize_llama()
            else:
                raise ValueError(f"Unsupported model type: {self.config.model_type}")
                
            self.logger.info(f"Successfully initialized {self.config.model_type} model")
            
        except Exception as e:
            self.logger.error(f"Error initializing model: {e}")
            raise

    def _initialize_openai(self):
        """Initialize OpenAI API client"""
        if not self.config.api_key:
            raise ValueError("API key required for OpenAI models")
            
        openai.api_key = self.config.api_key
        if self.config.api_base:
            openai.api_base = self.config.api_base

```
Lets generate texts this time
```python
def generate(self, 
                system_prompt: str, 
                user_prompt: str, 
                temperature: Optional[float] = None) -> str:
        """
        Generate text using the configured model
        
        Args:
            system_prompt: System context/instruction
            user_prompt: User input/query
            temperature: Optional temperature override
            
        Returns:
            str: Generated text response
        """
        temp = temperature if temperature is not None else self.config.temperature
        
        try:
            if self.config.model_type in [ModelType.GPT4, ModelType.GPT35]:
                return self._generate_openai(system_prompt, user_prompt, temp)
            else:
                return self._generate_local(system_prompt, user_prompt, temp)
                
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            raise

    def _generate_openai(self, 
                        system_prompt: str, 
                        user_prompt: str, 
                        temperature: float) -> str:
        """Generate text using OpenAI API"""
        model_name = "gpt-4" if self.config.model_type == ModelType.GPT4 else "gpt-3.5-turbo"
        
        response = openai.ChatCompletion.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=self.config.max_length
        )
        
        return response.choices[0].message.content


    def _initialize_llama(self):
        """Initialize Llama model with proper token configuration"""
        try:
            # Initialize tokenizer with proper defaults
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=True,
                padding_side="left"  # Important for attention mask alignment
            )
            
            # Ensure pad token is set correctly
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                
            # Initialize model with safe defaults
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch.float16 if self.config.device == "cuda" else torch.float32,
                device_map="auto",
                trust_remote_code=True,
                use_safetensors=True,  # Use safetensors to avoid weight issues
                low_cpu_mem_usage=True  # Help with memory management
            )
            
            # Ensure model and tokenizer vocab sizes match
            if len(self.tokenizer) != self.model.config.vocab_size:
                logging.warning(f"Tokenizer vocab size ({len(self.tokenizer)}) != Model vocab size ({self.model.config.vocab_size})")
                # Resize model embeddings to match tokenizer
                self.model.resize_token_embeddings(len(self.tokenizer))
            
            logging.info(f"Model initialized successfully with vocab size {len(self.tokenizer)}")
            
        except Exception as e:
            logging.error(f"Error initializing model: {e}")
            raise

    def _generate_local(self, 
                   system_prompt: str, 
                   user_prompt: str, 
                   temperature: float = 0.7,
                   max_new_tokens: Optional[int] = None) -> str:
        """Generate text using local models with enhanced handling"""
        try:
            # Format prompt according to Llama 3 chat template
            chat_template = f"""<s>[INST] <<SYS>>{system_prompt}<</SYS>>
    {user_prompt}[/INST]
    """
            # Tokenize with proper handling
            inputs = self.tokenizer(
                chat_template,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048,
                return_attention_mask=True,
            )
            
            # Move inputs to correct device
            input_ids = inputs['input_ids'].to(self.config.device)
            attention_mask = inputs['attention_mask'].to(self.config.device)

            # Set up generation config
            gen_config = {
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'max_new_tokens': max_new_tokens or 1024,
                'do_sample': temperature > 0,
                'temperature': temperature,
                'top_p': 0.9,
                'top_k': 50,
                'repetition_penalty': 1.1,
                'pad_token_id': self.tokenizer.pad_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'use_cache': True
            }

```
lets try chatgpt4 this time :)

```python
def main():
    Example 1: Using GPT-4
    gpt4_config = ModelConfig(
        model_type=ModelType.GPT4,
        api_key="your-api-key"
    )
    gpt4_client = LLMClient(config=gpt4_config)
```
define your own output:
```python
# Generate text
    system_prompt = "You are an outstanding clothing stylist. You are good at matching and designing eye-catching looks and keeping people at an appropriate temperature and comfort level."
    user_prompt = "What is the best outfit for me, today is 10 degrees Celsius?"
    
    response = llama_client.generate(system_prompt, user_prompt)
    print("========")
    print(response)

if __name__ == "__main__":
    main()
```

## what to do in the future:
We are planning to integrate the image recognize function within this final project.

## Outcome
![](outcome.png)

