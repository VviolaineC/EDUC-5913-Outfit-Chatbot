# EDUC-5913-Outfit-Chatbot
:)

## Introduction

ğŸ’–Hello everyone. The project of Haipei and I wants to establish a vertical chatbot system integrating multiple models for daily clothing matching. We combine text language models and image recognition capabilities to provide real-time feedback based on user-input images, temperatures within a limited time range, and dressing scenarios.

ğŸ’–å¤§å®¶å¥½ï¼Œæˆ‘å’Œæµ·åŸ¹çš„é¡¹ç›®æƒ³è¦å»ºç«‹ä¸€ä¸ªé›†æˆå¤šç§æ¨¡å‹çš„å‚ç›´é¢†åŸŸèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºæ—¥å¸¸æœè£…æ­é…ã€‚æˆ‘ä»¬ç»“åˆäº†æ–‡æœ¬è¯­è¨€æ¨¡å‹ä»¥åŠå›¾ç‰‡è¯†åˆ«èƒ½åŠ›ï¼Œä¾æ®ç”¨æˆ·è¾“å…¥çš„å›¾ç‰‡ã€é™å®šæ—¶é—´èŒƒå›´å†…çš„æ°”æ¸©ã€ç©¿æ­æ‰€éœ€åœºæ™¯æ¥è¿›è¡Œå®æ—¶åé¦ˆã€‚ 

ğŸ˜‡Since there are two of us writing code simultaneously, maintenance and collaborative construction are of great significance in code editing. For example, every time we add a new model type, we need to make manual modifications in multiple parts of the code, which makes it easy to miss something or introduce errors. 

ğŸ˜‡å› ä¸ºæˆ‘ä»¬æ˜¯ä¸¤ä¸ªäººåŒæ—¶å†™ä½œï¼Œæ‰€ä»¥ç»´æŠ¤å’Œå…±å»ºåœ¨ä»£ç ç¼–è¾‘ä¸­æ˜¾å¾—ååˆ†é‡è¦ã€‚è­¬å¦‚è¯´ï¼Œ å½“æˆ‘ä»¬æ¯æ¬¡æ–°å¢ä¸€ä¸ªæ¨¡å‹ç±»å‹ï¼Œéƒ½éœ€è¦åœ¨ä»£ç çš„å¤šå¤„æ‰‹åŠ¨ä¿®æ”¹ï¼Œå®¹æ˜“é—æ¼æˆ–å¼•å…¥é”™è¯¯ã€‚

## Potential issues
### ğŸ¤¯Potential Scenario 1 æ½œåœ¨åœºæ™¯1 
```python
# new additional model "gpt5"
if model_type == "gpt5":
print("Running GPT-5")
```
Issues é—®é¢˜æ‰€åœ¨:

â€¢ If these strings are used in dozens of places, we need to check one by one to see if they have all been updated.

â€¢ Without centralized management, it's very easy to miss something or cause logical conflicts. 

â€¢	å¦‚æœæœ‰å‡ åå¤„åœ°æ–¹ä½¿ç”¨äº†è¿™äº›å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬éœ€è¦é€ä¸€æ£€æŸ¥æ˜¯å¦éƒ½æ›´æ–°äº†ã€‚

â€¢	æ²¡æœ‰é›†ä¸­ç®¡ç†æ—¶ï¼Œå¾ˆå®¹æ˜“é—æ¼æˆ–å¯¼è‡´é€»è¾‘å†²çªã€‚

### ğŸ¤¯Potential Scenario 2 æ½œåœ¨åœºæ™¯2

When I want to deploy a local open-source model from Hugging Face by myself, my team doesn't recognize this small self-owned model. Using strings directly can't provide any context information, and at this time, the meaning of the code isn't clear enough. 

å½“æˆ‘æƒ³è¦è‡ªå·±éƒ¨ç½²ä¸€ä¸ªhugging face ä¸Šçš„æœ¬åœ°å¼€æºæ¨¡å‹ï¼Œ æˆ‘çš„team ä¸è®¤è¯†è¿™ä¸ªè‡ªæœ‰å°æ¨¡å‹ï¼Œ ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²æ— æ³•æä¾›ä»»ä½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ­¤æ—¶ä»£ç çš„å«ä¹‰ä¸å¤Ÿæ¸…æ™°ã€‚

```python
if model_type == "llama3":
print("Model is LLaMA-3")
```

Issues é—®é¢˜æ‰€åœ¨:

â€¢	What is "llama3"? Is it a variable? Or a specific value?

â€¢	"llama3" æ˜¯ä»€ä¹ˆï¼Ÿä¸€ä¸ªå˜é‡ï¼Ÿè¿˜æ˜¯ä¸€ä¸ªå…·ä½“çš„å€¼ï¼Ÿ

**âš ï¸Therefore, during the process of writing code, what we consider is not just the construction of the system. How to make our existing system more semantic (clear), safer and easier to maintain is of crucial importance. Hence, in the early stage of building the system, we introduced "Enum". Although the following piece of code is simple, its advantages will become more and more obvious as the program becomes more complex.**

**âš ï¸å› æ­¤ï¼Œåœ¨å†™ä»£ç çš„è¿‡ç¨‹ä¸­ï¼Œ æˆ‘ä»¬è€ƒè™‘çš„ä¸ä»…ä»…æ˜¯ç³»ç»Ÿçš„æ„å»ºï¼Œ å¦‚ä½•å°†æˆ‘ä»¬ç°æœ‰ç³»ç»Ÿæ„å»ºå¾—æ›´è¯­ä¹‰åŒ–ï¼ˆæ¸…æ™°ï¼‰ã€æ›´å®‰å…¨ã€æ›´æ˜“ç»´æŠ¤ï¼Œè‡³å…³é‡è¦ã€‚ æ‰€ä»¥ï¼Œåœ¨æ„å»ºç³»ç»Ÿçš„å‰æœŸï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œEnumâ€ã€‚ 
ä»¥ä¸‹æ®µä»£ç è™½ç„¶ç®€å•ï¼Œä½†å®ƒçš„å¥½å¤„ä¼šåœ¨ç¨‹åºå˜å¤æ‚æ—¶è¶Šæ¥è¶Šæ˜æ˜¾ã€‚**


```python
class ModelType(Enum):
    """Supported model types"""
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
```
## So, What is the Benefit of Using Enum?

ğŸ§ Define a set of fixed model types, which represent a set of options that won't change randomly.

ğŸ§ å®šä¹‰ä¸€ç»„å›ºå®šçš„æ¨¡å‹ç±»å‹ï¼Œè¿™äº›æ¨¡å‹ç±»å‹ä»£è¡¨äº†ä¸€ç»„ä¸ä¼šéšæ„å˜åŒ–çš„é€‰é¡¹ã€‚

ğŸ¤™ğŸ»Improve the readability of code

ğŸ¤™ğŸ»æé«˜ä»£ç çš„å¯è¯»æ€§

ğŸ‘ŒğŸ»Improve code security

ğŸ‘ŒğŸ»æé«˜ä»£ç å®‰å…¨æ€§

Besides the issue of string spelling, when using enums, IDEs or code checking tools will automatically prompt errors.

é™¤äº†å­—ç¬¦ä¸²æ‹¼å†™çš„é—®é¢˜ï¼Œ ç”¨æšä¸¾æ—¶ï¼ŒIDE æˆ–ä»£ç æ£€æŸ¥å·¥å…·ä¼šè‡ªåŠ¨æç¤ºé”™è¯¯
```python
if model_type == ModelType.GPT5:  # If GPT5 is not defined, an error will be reported here.
print("This is GPT-4")
```

ğŸ˜More convenient for expansion and maintenance.

ğŸ˜æ›´æ–¹ä¾¿çš„æ‰©å±•å’Œç»´æŠ¤

Suppose a new model needs to be added in the future, such as "gpt5". You can directly add a new member to the enumeration class: 

å‡è®¾ä»¥åéœ€è¦æ·»åŠ ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Œæ¯”å¦‚ "gpt5"ï¼Œå¯ä»¥ç›´æ¥åœ¨æšä¸¾ç±»ä¸­æ–°å¢æˆå‘˜ï¼š

```python
class ModelType(Enum):
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
GPT5 = "gpt5"  # newly added
```

## Scaling 

ğŸ˜†Similarly, the @dataclass we use later is also considered for the same reason: We hope it can simplify class definitions in the code and enhance its maintainability. 

ğŸ˜†åŒç†ï¼Œæˆ‘ä»¬åœ¨ä¹‹åä½¿ç”¨çš„@dataclassä¹Ÿæ˜¯æ‹¥æœ‰åŒæ ·çš„è€ƒè™‘ï¼š å¸Œæœ›ä»£ç ç®€åŒ–ç±»å®šä¹‰ã€å¢å¼ºå…¶å¯ç»´æŠ¤æ€§ã€‚

For example, without @dataclass, we would need to write these methods manually:

ä¾‹å¦‚ï¼Œå¦‚æœæ²¡æœ‰ @dataclassï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ç¼–å†™è¿™äº›æ–¹æ³•ï¼š

```python
class ModelConfig:
    def __init__(self, model_type, max_length=4096, temperature=0.7):
        self.model_type = model_type
        self.max_length = max_length
        self.temperature = temperature

    def __repr__(self):
        return f"ModelConfig(model_type={self.model_type}, max_length={self.max_length}, temperature={self.temperature})"

    def __eq__(self, other):
        if not isinstance(other, ModelConfig):
            return False
        return (self.model_type == other.model_type and
                self.max_length == other.max_length and
                self.temperature == other.temperature)
```

**However, after using @dataclass, the effect is exactly the same and the code is quite concise.**

**ä½†æœ‰äº†@dataclass ä¹‹åï¼Œ æ•ˆæœå®Œå…¨ç›¸åŒï¼Œä»£ç ç›¸å½“ç®€æ´ã€‚**

```python
from dataclasses import dataclass

@dataclass
class ModelConfig:
    model_type: str
    max_length: int = 4096
temperature: float = 0.7
```

## ModelConfig & LLMClient

Since we expect this system to integrate multiple models, we use a unified interface to initialize, manage and utilize various large language models (LLM). Here, we use ModelConfig to store the parameters for model initialization, and the LLMClient is employed to read the content of ModelConfig to load and use the models. 

ç”±äºæˆ‘ä»¬å¸Œæœ›æœ¬ç³»ç»Ÿé›†æˆå¤šç§æ¨¡å‹ï¼Œ å› æ­¤æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£æ¥åˆå§‹åŒ–ã€ç®¡ç†å’Œä½¿ç”¨å„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä½¿ç”¨ModelConfigè¿›è¡Œæ¨¡å‹åˆå§‹åŒ–çš„å‚æ•°å­˜å‚¨ã€ ç”¨LLMClientè¯»å– ModelConfig çš„å†…å®¹æ¥åŠ è½½å’Œä½¿ç”¨æ¨¡å‹ã€‚

```python
class LLMClient:
    """Generic LLM client supporting multiple model types"""
    
    def __init__(self, config: ModelConfig):
        """
        Initialize LLM client with specified configuration
        
        Args:
            config: ModelConfig instance with model settings
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.tokenizer = None
        
        # Initialize the specified model type
        self._initialize_model()
```

## Model Initialization 

### The Problem

When we first implemented model initialization, we ran into a couple of issues:

1.	No strict validation
2.	Risk of crashes

### Why Non-Modular Design Doesnâ€™t Work!

If you donâ€™t use a modular approach, you end up putting all the logic for selecting and initializing models directly in the main flow. Hereâ€™s why thatâ€™s a bad idea:

1.	Redundant code

2.	Hard to maintain

3.	Scalability issues

### Non-Modular Example

Hereâ€™s what it looked like without modular design:

```python
try:
    if config.model_type in [ModelType.GPT4, ModelType.GPT35]:
        if not config.api_key:
            raise ValueError("API key required for OpenAI models")
        openai.api_key = config.api_key
        if config.api_base:
            openai.api_base = config.api_base
        logger.info(f"Successfully initialized OpenAI model: {config.model_type}")
        
    elif config.model_type == ModelType.LLAMA3:
        # Logic for initializing LLAMA3
        logger.info("Successfully initialized LLAMA3 model")
        
    else:
        raise ValueError(f"Unsupported model type: {config.model_type}")
        
except Exception as e:
    logger.error(f"Error initializing model: {e}")
    raise
```

As you can see, all the initialization logic is crammed into one place. Itâ€™s messy, hard to read, and risky when it comes to adding or modifying features.


### The Modular Solution

To fix this, we switched to a modular design. Hereâ€™s how it works:


1.	Validate model_type:

o	If itâ€™s GPT4 or GPT3.5, we call _initialize_openai to handle OpenAI models.

o	If itâ€™s LLAMA3, we call _initialize_llama to handle the Llama model.

o	If itâ€™s unsupported, we raise an exception and log the error.

2.	Log the result: We log what happened whether itâ€™s a success or failure.








## About Image upload å…³äºä¸Šä¼ å›¾åƒ

Next, we defined a method named _encode_image, whose function is to convert an image file into a Base64-encoded string. We send the corresponding image data through the API interface. The specific process is as follows: The user uploads an image â†’ The image is encoded into a Base64 string and embedded in the JSON request â†’ The server receives the request and decodes the Base64 string, converting it into a format that can be processed by the Convolutional Neural Network (CNN). 

æ¥ç€ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæ–¹æ³• _encode_imageï¼Œå®ƒçš„ä½œç”¨æ˜¯ å°†å›¾åƒæ–‡ä»¶è½¬æ¢ä¸º Base64 ç¼–ç çš„å­—ç¬¦ä¸²ã€‚ æˆ‘ä»¬é€šè¿‡API æ¥å£å‘é€ç›¸å¯¹åº”çš„å›¾åƒæ•°æ®ã€‚å…·ä½“è·¯å¾„ä¸ºï¼šç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ğŸ‘‰å›¾ç‰‡è¢«ç¼–ç ä¸º Base64 å­—ç¬¦ä¸²ï¼ŒåµŒå…¥åˆ° JSON è¯·æ±‚ä¸­ğŸ‘‰æœåŠ¡ç«¯æ¥æ”¶è¯·æ±‚å¹¶è§£ç  Base64 å­—ç¬¦ä¸²ï¼Œå°†å…¶è½¬æ¢ä¸º CNN å¯å¤„ç†çš„æ ¼å¼ã€‚

```python
    def _encode_image(self, image_path: str) -> str:
        """
        Encode image to base64 string
        
        Args:
            image_path: Path to image file
            
        Returns:
            str: Base64 encoded image
        """
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
```

## Prompt engineering

Last but not least, We have defined system prompt, user_prompt, image_paths and temperature.

```python
    def generate(self, 
                system_prompt: str, 
                user_prompt: str, 
                image_paths: Optional[List[str]] = None,
                temperature: Optional[float] = None) -> str:
```

## Outcome!

é€šè¿‡ GPT-4V æ¨¡å‹å®ç°ä¸€ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œç»“åˆæ–‡æœ¬æç¤ºå’Œå›¾åƒè¾“å…¥ï¼Œä¸ºç”¨æˆ·æä¾›åŸºäºæ¸©åº¦å’Œåœºæ™¯çš„æœè£…æ­é…å»ºè®®ã€‚

Implement a multi-modal task through the GPT-4V model, combining text prompts and image inputs to provide users with clothing matching suggestions based on temperature and scenarios. 

```python
gpt4v_config = ModelConfig(
        model_type=ModelType.GPT4V,
        api_key="API-key"
    )
    gpt4v_client = LLMClient(config=gpt4v_config)

    
    system_prompt = "You are an outstanding clothing stylist. You are good at matching and designing eye-catching looks and keeping people at an appropriate temperature and comfort level."
    user_prompt = "These are the clothes I need for dressing up within a week. The average temperature this week is between 10 and 20 degrees Celsius. There are no special activities this week, and the main activity is going to work. Please help me match the clothes according to my pictures, the temperature and the scene."
    image_paths = ["imgs/croptop.png", "imgs/hoodie.webp", "imgs/jacket.png", "imgs/shirt.png", "imgs/trenchcoat.png"]
    
    response = gpt4v_client.generate(system_prompt, user_prompt, image_paths)
    print("========")
    print(response)
```
