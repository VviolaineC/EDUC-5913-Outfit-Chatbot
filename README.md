# EDUC-5913-Outfit-Chatbot
:)

## Introduction

ðŸ’–Hello everyone. The project of Haipei and I wants to establish a vertical chatbot system integrating multiple models for daily clothing matching. We combine text language models and image recognition capabilities to provide real-time feedback based on user-input images, temperatures within a limited time range, and dressing scenarios.

ðŸ’–å¤§å®¶å¥½ï¼Œæˆ‘å’Œæµ·åŸ¹çš„é¡¹ç›®æƒ³è¦å»ºç«‹ä¸€ä¸ªé›†æˆå¤šç§æ¨¡åž‹çš„åž‚ç›´é¢†åŸŸèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºŽæ—¥å¸¸æœè£…æ­é…ã€‚æˆ‘ä»¬ç»“åˆäº†æ–‡æœ¬è¯­è¨€æ¨¡åž‹ä»¥åŠå›¾ç‰‡è¯†åˆ«èƒ½åŠ›ï¼Œä¾æ®ç”¨æˆ·è¾“å…¥çš„å›¾ç‰‡ã€é™å®šæ—¶é—´èŒƒå›´å†…çš„æ°”æ¸©ã€ç©¿æ­æ‰€éœ€åœºæ™¯æ¥è¿›è¡Œå®žæ—¶åé¦ˆã€‚ 

ðŸ˜‡Since there are two of us writing code simultaneously, maintenance and collaborative construction are of great significance in code editing. For example, every time we add a new model type, we need to make manual modifications in multiple parts of the code, which makes it easy to miss something or introduce errors. 

ðŸ˜‡å› ä¸ºæˆ‘ä»¬æ˜¯ä¸¤ä¸ªäººåŒæ—¶å†™ä½œï¼Œæ‰€ä»¥ç»´æŠ¤å’Œå…±å»ºåœ¨ä»£ç ç¼–è¾‘ä¸­æ˜¾å¾—ååˆ†é‡è¦ã€‚è­¬å¦‚è¯´ï¼Œ å½“æˆ‘ä»¬æ¯æ¬¡æ–°å¢žä¸€ä¸ªæ¨¡åž‹ç±»åž‹ï¼Œéƒ½éœ€è¦åœ¨ä»£ç çš„å¤šå¤„æ‰‹åŠ¨ä¿®æ”¹ï¼Œå®¹æ˜“é—æ¼æˆ–å¼•å…¥é”™è¯¯ã€‚

## Potential issues
### ðŸ¤¯Potential Scenario 1 æ½œåœ¨åœºæ™¯1 
```python
# new additional model "gpt5"
if model_type == "gpt5":
print("Running GPT-5")
```
Issues é—®é¢˜æ‰€åœ¨:

â€¢ If these strings are used in dozens of places, we need to check one by one to see if they have all been updated.

â€¢ Without centralized management, it's very easy to miss something or cause logical conflicts. 

â€¢	å¦‚æžœæœ‰å‡ åå¤„åœ°æ–¹ä½¿ç”¨äº†è¿™äº›å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬éœ€è¦é€ä¸€æ£€æŸ¥æ˜¯å¦éƒ½æ›´æ–°äº†ã€‚

â€¢	æ²¡æœ‰é›†ä¸­ç®¡ç†æ—¶ï¼Œå¾ˆå®¹æ˜“é—æ¼æˆ–å¯¼è‡´é€»è¾‘å†²çªã€‚

### ðŸ¤¯Potential Scenario 2 æ½œåœ¨åœºæ™¯2

When I want to deploy a local open-source model from Hugging Face by myself, my team doesn't recognize this small self-owned model. Using strings directly can't provide any context information, and at this time, the meaning of the code isn't clear enough. 

å½“æˆ‘æƒ³è¦è‡ªå·±éƒ¨ç½²ä¸€ä¸ªhugging face ä¸Šçš„æœ¬åœ°å¼€æºæ¨¡åž‹ï¼Œ æˆ‘çš„team ä¸è®¤è¯†è¿™ä¸ªè‡ªæœ‰å°æ¨¡åž‹ï¼Œ ç›´æŽ¥ä½¿ç”¨å­—ç¬¦ä¸²æ— æ³•æä¾›ä»»ä½•ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ­¤æ—¶ä»£ç çš„å«ä¹‰ä¸å¤Ÿæ¸…æ™°ã€‚

```python
if model_type == "llama3":
print("Model is LLaMA-3")
```

Issues é—®é¢˜æ‰€åœ¨:

â€¢	What is "llama3"? Is it a variable? Or a specific value?

â€¢	"llama3" æ˜¯ä»€ä¹ˆï¼Ÿä¸€ä¸ªå˜é‡ï¼Ÿè¿˜æ˜¯ä¸€ä¸ªå…·ä½“çš„å€¼ï¼Ÿ

**âš ï¸Therefore, during the process of writing code, what we consider is not just the construction of the system. How to make our existing system more semantic (clear), safer and easier to maintain is of crucial importance. Hence, in the early stage of building the system, we introduced "Enum". Although the following piece of code is simple, its advantages will become more and more obvious as the program becomes more complex.**

**âš ï¸å› æ­¤ï¼Œåœ¨å†™ä»£ç çš„è¿‡ç¨‹ä¸­ï¼Œ æˆ‘ä»¬è€ƒè™‘çš„ä¸ä»…ä»…æ˜¯ç³»ç»Ÿçš„æž„å»ºï¼Œ å¦‚ä½•å°†æˆ‘ä»¬çŽ°æœ‰ç³»ç»Ÿæž„å»ºå¾—æ›´è¯­ä¹‰åŒ–ï¼ˆæ¸…æ™°ï¼‰ã€æ›´å®‰å…¨ã€æ›´æ˜“ç»´æŠ¤ï¼Œè‡³å…³é‡è¦ã€‚ æ‰€ä»¥ï¼Œåœ¨æž„å»ºç³»ç»Ÿçš„å‰æœŸï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œEnumâ€ã€‚ 
ä»¥ä¸‹æ®µä»£ç è™½ç„¶ç®€å•ï¼Œä½†å®ƒçš„å¥½å¤„ä¼šåœ¨ç¨‹åºå˜å¤æ‚æ—¶è¶Šæ¥è¶Šæ˜Žæ˜¾ã€‚**


```python
class ModelType(Enum):
    """Supported model types"""
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
```
## So, What is the Benefit of Using Enum?

ðŸ§ Define a set of fixed model types, which represent a set of options that won't change randomly.

ðŸ§ å®šä¹‰ä¸€ç»„å›ºå®šçš„æ¨¡åž‹ç±»åž‹ï¼Œè¿™äº›æ¨¡åž‹ç±»åž‹ä»£è¡¨äº†ä¸€ç»„ä¸ä¼šéšæ„å˜åŒ–çš„é€‰é¡¹ã€‚

ðŸ¤™ðŸ»Improve the readability of code

ðŸ¤™ðŸ»æé«˜ä»£ç çš„å¯è¯»æ€§

ðŸ‘ŒðŸ»Improve code security

ðŸ‘ŒðŸ»æé«˜ä»£ç å®‰å…¨æ€§

Besides the issue of string spelling, when using enums, IDEs or code checking tools will automatically prompt errors.

é™¤äº†å­—ç¬¦ä¸²æ‹¼å†™çš„é—®é¢˜ï¼Œ ç”¨æžšä¸¾æ—¶ï¼ŒIDE æˆ–ä»£ç æ£€æŸ¥å·¥å…·ä¼šè‡ªåŠ¨æç¤ºé”™è¯¯
```python
if model_type == ModelType.GPT5:  # If GPT5 is not defined, an error will be reported here.
print("This is GPT-4")
```

ðŸ˜More convenient for expansion and maintenance.

ðŸ˜æ›´æ–¹ä¾¿çš„æ‰©å±•å’Œç»´æŠ¤

Suppose a new model needs to be added in the future, such as "gpt5". You can directly add a new member to the enumeration class: 

å‡è®¾ä»¥åŽéœ€è¦æ·»åŠ ä¸€ä¸ªæ–°çš„æ¨¡åž‹ï¼Œæ¯”å¦‚ "gpt5"ï¼Œå¯ä»¥ç›´æŽ¥åœ¨æžšä¸¾ç±»ä¸­æ–°å¢žæˆå‘˜ï¼š

```python
class ModelType(Enum):
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
GPT5 = "gpt5"  # newly added
```

## Scaling 

ðŸ˜†Similarly, the @dataclass we use later is also considered for the same reason: We hope it can simplify class definitions in the code and enhance its maintainability. 

ðŸ˜†åŒç†ï¼Œæˆ‘ä»¬åœ¨ä¹‹åŽä½¿ç”¨çš„@dataclassä¹Ÿæ˜¯æ‹¥æœ‰åŒæ ·çš„è€ƒè™‘ï¼š å¸Œæœ›ä»£ç ç®€åŒ–ç±»å®šä¹‰ã€å¢žå¼ºå…¶å¯ç»´æŠ¤æ€§ã€‚

For example, without @dataclass, we would need to write these methods manually:
ä¾‹å¦‚ï¼Œå¦‚æžœæ²¡æœ‰ @dataclassï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ç¼–å†™è¿™äº›æ–¹æ³•ï¼š

```python
class ModelConfig:
    def __init__(self, model_type, max_length=4096, temperature=0.7):
        self.model_type = model_type
        self.max_length = max_length
        self.temperature = temperature

    def __repr__(self):
        return f"ModelConfig(model_type={self.model_type}, max_length={self.max_length}, temperature={self.temperature})"

    def __eq__(self, other):
        if not isinstance(other, ModelConfig):
            return False
        return (self.model_type == other.model_type and
                self.max_length == other.max_length and
                self.temperature == other.temperature)
```

**However, after using @dataclass, the effect is exactly the same and the code is quite concise.**

**ä½†æœ‰äº†@dataclass ä¹‹åŽï¼Œ æ•ˆæžœå®Œå…¨ç›¸åŒï¼Œä»£ç ç›¸å½“ç®€æ´ã€‚**

```python
from dataclasses import dataclass

@dataclass
class ModelConfig:
    model_type: str
    max_length: int = 4096
temperature: float = 0.7
```



```python
from typing import Optional
from enum import Enum
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import openai
from dataclasses import dataclass
```
Define the name in case of miswritting
```python
class ModelType(Enum):
    """Supported model types"""
    LLAMA3 = "llama3"
    GPT4 = "gpt4"
    GPT35 = "gpt3.5"
```
Class definition using the Python @dataclass decorator for configuring the initialization parameters of an AI model.
```python
class ModelConfig:
    """Configuration for model initialization"""
    model_type: ModelType
    model_path: Optional[str] = None  # Local path or HuggingFace model ID
    api_key: Optional[str] = None
    api_base: Optional[str] = None
    max_length: int = 4096
    temperature: float = 0.7
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
```
Initialize the appropriate model based on configuration
```python
    def _initialize_model(self):
        """Initialize the appropriate model based on configuration"""
        try:
            if self.config.model_type in [ModelType.GPT4, ModelType.GPT35]:
                self._initialize_openai()
            elif self.config.model_type == ModelType.LLAMA3:
                self._initialize_llama()
            else:
                raise ValueError(f"Unsupported model type: {self.config.model_type}")
                
            self.logger.info(f"Successfully initialized {self.config.model_type} model")
            
        except Exception as e:
            self.logger.error(f"Error initializing model: {e}")
            raise

    def _initialize_openai(self):
        """Initialize OpenAI API client"""
        if not self.config.api_key:
            raise ValueError("API key required for OpenAI models")
            
        openai.api_key = self.config.api_key
        if self.config.api_base:
            openai.api_base = self.config.api_base

```
Lets generate texts this time
```python
def generate(self, 
                system_prompt: str, 
                user_prompt: str, 
                temperature: Optional[float] = None) -> str:
        """
        Generate text using the configured model
        
        Args:
            system_prompt: System context/instruction
            user_prompt: User input/query
            temperature: Optional temperature override
            
        Returns:
            str: Generated text response
        """
        temp = temperature if temperature is not None else self.config.temperature
        
        try:
            if self.config.model_type in [ModelType.GPT4, ModelType.GPT35]:
                return self._generate_openai(system_prompt, user_prompt, temp)
            else:
                return self._generate_local(system_prompt, user_prompt, temp)
                
        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            raise

    def _generate_openai(self, 
                        system_prompt: str, 
                        user_prompt: str, 
                        temperature: float) -> str:
        """Generate text using OpenAI API"""
        model_name = "gpt-4" if self.config.model_type == ModelType.GPT4 else "gpt-3.5-turbo"
        
        response = openai.ChatCompletion.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            max_tokens=self.config.max_length
        )
        
        return response.choices[0].message.content


    def _initialize_llama(self):
        """Initialize Llama model with proper token configuration"""
        try:
            # Initialize tokenizer with proper defaults
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=True,
                padding_side="left"  # Important for attention mask alignment
            )
            
            # Ensure pad token is set correctly
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                
            # Initialize model with safe defaults
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch.float16 if self.config.device == "cuda" else torch.float32,
                device_map="auto",
                trust_remote_code=True,
                use_safetensors=True,  # Use safetensors to avoid weight issues
                low_cpu_mem_usage=True  # Help with memory management
            )
            
            # Ensure model and tokenizer vocab sizes match
            if len(self.tokenizer) != self.model.config.vocab_size:
                logging.warning(f"Tokenizer vocab size ({len(self.tokenizer)}) != Model vocab size ({self.model.config.vocab_size})")
                # Resize model embeddings to match tokenizer
                self.model.resize_token_embeddings(len(self.tokenizer))
            
            logging.info(f"Model initialized successfully with vocab size {len(self.tokenizer)}")
            
        except Exception as e:
            logging.error(f"Error initializing model: {e}")
            raise

    def _generate_local(self, 
                   system_prompt: str, 
                   user_prompt: str, 
                   temperature: float = 0.7,
                   max_new_tokens: Optional[int] = None) -> str:
        """Generate text using local models with enhanced handling"""
        try:
            # Format prompt according to Llama 3 chat template
            chat_template = f"""<s>[INST] <<SYS>>{system_prompt}<</SYS>>
    {user_prompt}[/INST]
    """
            # Tokenize with proper handling
            inputs = self.tokenizer(
                chat_template,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048,
                return_attention_mask=True,
            )
            
            # Move inputs to correct device
            input_ids = inputs['input_ids'].to(self.config.device)
            attention_mask = inputs['attention_mask'].to(self.config.device)

            # Set up generation config
            gen_config = {
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'max_new_tokens': max_new_tokens or 1024,
                'do_sample': temperature > 0,
                'temperature': temperature,
                'top_p': 0.9,
                'top_k': 50,
                'repetition_penalty': 1.1,
                'pad_token_id': self.tokenizer.pad_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'use_cache': True
            }

```
lets try chatgpt4 this time :)

```python
def main():
    Example 1: Using GPT-4
    gpt4_config = ModelConfig(
        model_type=ModelType.GPT4,
        api_key="your-api-key"
    )
    gpt4_client = LLMClient(config=gpt4_config)
```
define your own output:
```python
# Generate text
    system_prompt = "You are an outstanding clothing stylist. You are good at matching and designing eye-catching looks and keeping people at an appropriate temperature and comfort level."
    user_prompt = "What is the best outfit for me, today is 10 degrees Celsius?"
    
    response = llama_client.generate(system_prompt, user_prompt)
    print("========")
    print(response)

if __name__ == "__main__":
    main()
```

## what to do in the future:
We are planning to integrate the image recognize function within this final project.

## Outcome
![](outcome.png)

